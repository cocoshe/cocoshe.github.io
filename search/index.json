[{"content":"VAE 参考博客\n一些解读\nPCA  vae \n PCA的解释：原向量x乘以矩阵W得到中间编码c，再乘以W的转置，得到x head，得到的x head希望与原x越接近越好，有一点要注意，从x到c的变换过程是线性的。\n Deep Auto Encoder  和PCA类似，只是网络层数更深，变换是非线性的（因为可以加入一些非线性的激活函数），Deep Auto Encoder变成成了如下的样子：\n  \n中间有个很窄的hidden layer的输出就是压缩之后的code，当然以bottle layer对称的W不必要互为转置，也不要求一定要用RBM初始化参数，直接train，效果也很好。\n下面来看一下，对于MNIST手写数字数据集用像素点、PCA、Deep Auto Encoder三种方式分别做tSNE的展现图\n \n右上角为deep auto encoder之后做tSNE，所有的数字都分的很开，效果比较好。\n 总结一下，PCA和Deep auto encoder所做的事都类似，就是把原数据压缩到一个低维向量，让后再反解回来，反解回来的值希望与原来的值越接近越好。\nPCA像是比较基础，然后Deep auto encoder像是对PCA进行魔改\n Auto Encoder用于异常检测 ​ 对于自动编码器用于异常检测，可以参考《Variational Autoencoder based Anomaly Detection using Reconstruction Probability》这篇论文 ，论文地址：http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf ，论文标题大概可以这样翻译：基于变分自动编码器重建概率的异常检测。 ​ 文中不是直接从变分自动编码器切入，而是先介绍自动编码器，论文言简意赅，我们先来看看论文中对Auto Encoder的训练过程的描述\n下面来看，如果将Auto Encoder用于异常检测，还是先看公式\n \n 就是先训练一个encode，然后进行推理，得到loss，然后反向传播\n  \n说明：\n 先用正常的数据集训练一个Auto Encoder 用训练出的Auto Encoder计算异常数据的重建误差，重建误差大于某个阀值α，则为异常，否则则正常。  文中有这样一段描述：\n Autoencoder based anomaly detection is a deviation based anomaly detection method using semi-supervised learning. It uses the reconstruction error as the anomaly score. Data points with high reconstruction are considered to be anomalies.Only data with normal instances are used to train the autoencoder\n 这段描述指出了两点：\n1、半监督，用正常的数据训练一个Auto Encoder\n2、重建误差高的数据为异常数据\n普通Deep Auto Encoder有个缺陷，通俗的话来讲就是模型看过的类似的数据它知道，模型没看过的数据，几乎不可能能知道，那变分编码器，就可能解决了一部分问题，通过用分布相似来解释这个问题，请看下面的公式：\n训练：\n \n 补充E等式右边第一项知识：博客连接\nqφ（z | x）是近似后验，pθ（z）是潜在变量z的先验分布。论文翻译\n今天开始来讲相对熵，我们知道信息熵反应了一个系统的有序化程度，一个系统越是有序，那么它的信息熵就越低，反之就越高。下面是熵的定义 ​ 如果一个随机变量X的可能取值为 ，对应的概率为 ，则随机变量X的熵定义为 \n有了信息熵的定义，接下来开始学习相对熵。\n  相对熵的认识 相对熵又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度（即KL散度）等。设p(x)和q(x) 是X取值的两个概率概率分布，则p对q的相对熵为 \n在一定程度上，熵可以度量两个随机变量的距离。KL散度是两个概率分布P和Q差别的非对称性的度量。KL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。\n  　2. 相对熵的性质 相对熵（KL散度）有两个主要的性质。如下\n （1）尽管KL散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即 ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMwLmNuYmxvZ3MuY29tL2Jsb2cvNTcxMjI3LzIwMTUwMS8wNzIwNDIxMjU5Mzc0MTIucG5n?x-oss-process=image/format,png) （2）相对熵的值为非负值，即![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMwLmNuYmxvZ3MuY29tL2Jsb2cvNTcxMjI3LzIwMTUwMS8wNzIwNDQzODcwMzc3NzQucG5n?x-oss-process=image/format,png)    说人话，就是Dkl叫KL散度，第一项是用来衡量距离的一种指标  整个训练的过程用通俗的话说明一下，首先从标准正态分布中随机L个数据和原x经过hφ函数后得到隐含变量z，注意这里是为每一个样本x都随机L个数据来生成z，loss函数告诉我们两件事情：\n第一件，希望z的分布和给定了x的条件下z的分布越接近越好\n第二件，希望给定了z的条件下反解回来的x和原分布越接近越好。本质上而言，VAE是给原x加上了随机噪声，同时希望可以反解回原来的值\n中间隐含变量就非常神奇了，可以是高斯分布，也可以是伯努利分布，用概率分布来编码，在自动生成中有妙用。 推理：\n \n原文博主的一些思考（搬）： 论文中只给出了可以这样做以及这样做的效果，但是论文中没有解释为什么这样做。\n其实这个问题比较好类比，训练数据都是正常的，就好比一个人生活的圈子里只有猫，突然来了一条狗，他就肯定不认识了，只知道和猫不同，那么通过数学中的线性回归来类推一下，根据一批正常点拟合了一条直线，有一个游离于这个群体的点，自然离这条直线很远。同样的可以用数据边界来解释，神经网络见过了大量的正常数据，便学习到了数据的边界，游离于边界外的数据，自然也可以摘出来。\n但是同样有一个问题，既然原始样本集里已经有正常数据和异常数据了，通过有监督训练一个分类模型就够了。但是真实的场景里，我们是不知道怎么标注数据的，如果说可以标注，肯定是基于人工指定的规则，既然有了规则，就基于规则写if else（这里是针对数据维度比较小的场景）了，何必要机器来学习，但是规则制定的是否正确还两说，比方说金融领域常见的风险评估，某个用户是否可以放贷给他，已知该用户的各种信息，例如：年龄、性别、职业、收入、信用卡账单等等，这里只是举一个小例子。\n以上的疑问，总结起来就两点\n1、如何标注数据，到底是先有鸡还是先有蛋\n2、基于定死的规则标注的数据是否正确可用\n那么，能不能完全无监督的让机器学习出识别这些异常数据的规则，我觉得通过Auto Encoder是可以做到的，首先，对于异常识别的场景，正常的样本数肯定占大多数，异常的只是少数，把整个样本集完全扔给Deep Auto Encoder，让他去学习，同样可以找出异常数据。\n个人总结 对比来看三种算法\n PCA：简单地进行降维采样，让低纬度的参数去学习“生成原始数据”的方法 Deep Auto Encoder：对PCA进行拓展魔改，由原来单纯的线性变换拓展为非线性，对称拓展为非对称，并且可以自己设置中间的维度变换过程 VAE：在前面的基础上，随机加入噪声，使得模型能够有较好的泛化性  ","date":"2021-11-26T11:02:10+08:00","image":"https://pic1.zhimg.com/80/v2-d72d012f50ad64ff3a7de3e7e6c56a64_720w.png","permalink":"https://example.com/p/vae/","title":"VAE"},{"content":"手撸一下Resnet18  模型结构参考 \n模型细节可以参考CSDN上的一篇博客\nimport torch import torch.nn as nn class Identity(nn.Module): def __init__(self): super(Identity, self).__init__() def forward(self, x): return x class Block(nn.Module): def __init__(self, in_dim, dim, kernal_size, stride): super(Block, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_dim, out_channels=dim, kernel_size=kernal_size, stride=stride, padding=1) self.bn1 = nn.BatchNorm2d(dim) self.relu1 = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=kernal_size, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(dim) self.conv3 = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=kernal_size, stride=1, padding=1) self.bn3 = nn.BatchNorm2d(dim) self.relu2 = nn.ReLU() self.conv4 = nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=kernal_size, stride=1, padding=1) self.bn4 = nn.BatchNorm2d(dim) # 维度不一致有两种情况，可能是stride改变了w*h，也可能是conv层改变了维度 if stride == 2 or in_dim != dim: self.downsample = nn.Sequential(*[ nn.Conv2d(in_dim, dim, kernel_size=1, stride=stride), nn.BatchNorm2d(dim) ]) else: self.downsample = Identity() # 写一个什么都不干的类，好处是在forward里面少了ifelse的判断 def forward(self, x): h = x x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) x = self.conv2(x) x = self.bn2(x) # Identity(x) identity = self.downsample(h) # h和x的维度可能不一致，因为in_dim不一定等于dim(out_dim) x = x + identity x = self.conv3(x) x = self.bn3(x) x = self.relu2(x) x = self.conv4(x) x = self.bn4(x) return x class ResNet18(nn.Module): def __init__(self, in_dim=64, num_class=10): # 以十分类为例 super(ResNet18, self).__init__() self.in_dim = in_dim # stem self.conv1 = nn.Conv2d(in_channels=3, out_channels=in_dim, kernel_size=7, # 7有点大了，注意输入图片要大一些 stride=2, padding=3) self.relu = nn.ReLU() self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # res block self.layer1 = self.generateLayer(dim=64, kernal_size=3, stride=2) self.layer2 = self.generateLayer(dim=128, kernal_size=3, stride=1) self.layer3 = self.generateLayer(dim=256, kernal_size=3, stride=1) self.layer4 = self.generateLayer(dim=512, kernal_size=3, stride=1) # head self.avgpool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Linear(in_features=512, out_features=10) def generateLayer(self, dim, kernal_size, stride): layer_list = [] layer_list.append(Block(self.in_dim, dim, kernal_size, stride)) self.in_dim = dim layer_list.append(Block(self.in_dim, dim, kernal_size, 1)) return nn.Sequential(*layer_list) def forward(self, x): # stem x = self.conv1(x) x = self.relu(x) x = self.maxpool(x) # res block x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) # head x = self.avgpool(x) # Batch * 1 * 1 * C x = x.flatten(1) # Batch * C x = self.fc(x) # Batch * num_class return x def main(): t = torch.randn([4, 3, 224, 224]) model = ResNet18() out = model(t) print(out.shape) return if __name__ == \u0026#34;__main__\u0026#34;: main() 值得注意的是使用downsample处理维度问题，利用identity类搞一个什么都不干的东西，可以避免在forward中进行if-else判断\n之后写一写train的部分吧，然后上数据集测一下效果\n","date":"2021-11-26T01:27:04+08:00","image":"https://img-blog.csdnimg.cn/20200825153528831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjg5OTYyNw==,size_16,color_FFFFFF,t_70#pic_center","permalink":"https://example.com/p/%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%8Bresnet18/","title":"手撸一下Resnet18"},{"content":"Scrapy Scrapy is written in pure Python and depends on a few key Python packages (among others):\n lxml, an efficient XML and HTML parser parsel, an HTML/XML data extraction library written on top of lxml, w3lib, a multi-purpose helper for dealing with URLs and web page encodings twisted, an asynchronous networking framework cryptography and pyOpenSSL, to deal with various network-level security needs  官方doc\n 先scrapy startproject tutorial下好tutorial项目，然后自己配置一下venv进行pip3 install scrapy\n Get start import scrapy class QuotesSpider(scrapy.Spider): name = \u0026#34;quotes\u0026#34; def start_requests(self): urls = [ \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39;, \u0026#39;http://quotes.toscrape.com/page/2/\u0026#39;, ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(\u0026#34;/\u0026#34;)[-2] filename = f\u0026#39;quotes-{page}.html\u0026#39; with open(filename, \u0026#39;wb\u0026#39;) as f: f.write(response.body) self.log(f\u0026#39;Saved file {filename}\u0026#39;)  主要就是写一个类来继承scrapy.Spider类，然后完成一些必要的属性和方法的实现，如name,start_requests,parse等，都要实现特定的功能\n 然后在根目录下运行scrapy crawl quotes得到如下俩html文件\n. ├── bin ├── include ├── lib ├── lib64 -\u0026gt; lib ├── pyvenv.cfg ├── quotes-1.html ├── quotes-2.html ├── scrapy.cfg ├── share └── tutorial start_requests可以用一个start_urls的list来代替，如下：\nimport scrapy class QuotesSpider(scrapy.Spider): name = \u0026#34;quotes\u0026#34; start_urls = [ \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39;, \u0026#39;http://quotes.toscrape.com/page/2/\u0026#39;, ] def parse(self, response): page = response.url.split(\u0026#34;/\u0026#34;)[-2] filename = f\u0026#39;quotes-{page}.html\u0026#39; with open(filename, \u0026#39;wb\u0026#39;) as f: f.write(response.body) ps:f'表示之后的字符串可以直接通过大括号{}的方式进行传参~（例如上面的page就是'1',\u0026lsquo;2\u0026rsquo;）\nExtracting data 上面爬到的都是html,接下来要对数据进行提取\n下面利用shell进行一些测试\n发送请求，并且进入shell\nscrapy shell \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39; 下面对response中的title进行分析\n 直接取  \u0026gt;\u0026gt;\u0026gt; response.css(\u0026#39;title\u0026#39;) [\u0026lt;Selector xpath=\u0026#39;descendant-or-self::title\u0026#39; data=\u0026#39;\u0026lt;title\u0026gt;Quotes to Scrape\u0026lt;/title\u0026gt;\u0026#39;\u0026gt;] ::text去掉data的\u0026lt;title\u0026gt;标签  \u0026gt;\u0026gt;\u0026gt; response.css(\u0026#39;title::text\u0026#39;) [\u0026lt;Selector xpath=\u0026#39;descendant-or-self::title/text()\u0026#39; data=\u0026#39;Quotes to Scrape\u0026#39;\u0026gt;] getall()去掉前面的selector  \u0026gt;\u0026gt;\u0026gt; response.css(\u0026#39;title::text\u0026#39;).getall() [\u0026#39;Quotes to Scrape\u0026#39;] get()得到第一个selector  The other thing is that the result of calling .getall() is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n\u0026gt;\u0026gt;\u0026gt; response.css(\u0026#39;title::text\u0026#39;).get() \u0026#39;Quotes to Scrape\u0026#39; 或者这样操作\n\u0026gt;\u0026gt;\u0026gt; response.css('title::text')[0].get() 'Quotes to Scrape' 但是有缺陷，还是直接用get()好，因为这个可能会下标错误(如果是多个selector的话，用get()是默认返回第0个的)\n Besides the getall() and get() methods, you can also use the re() method to extract using\n除了getall、get等方法，还有一个re()方法可以根据正则表达式提取想要的数据\n example:\n对一个名人名言网站进行爬取，先请求一下得到response等信息\nscrapy shell \u0026#39;http://quotes.toscrape.com\u0026#39; 爬到的html大概是这样的(对于第一句话)\n\u0026lt;div class=\u0026#34;quote\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;text\u0026#34;\u0026gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt; by \u0026lt;small class=\u0026#34;author\u0026#34;\u0026gt;Albert Einstein\u0026lt;/small\u0026gt; \u0026lt;a href=\u0026#34;/author/Albert-Einstein\u0026#34;\u0026gt;(about)\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;div class=\u0026#34;tags\u0026#34;\u0026gt; Tags: \u0026lt;a class=\u0026#34;tag\u0026#34; href=\u0026#34;/tag/change/page/1/\u0026#34;\u0026gt;change\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;tag\u0026#34; href=\u0026#34;/tag/deep-thoughts/page/1/\u0026#34;\u0026gt;deep-thoughts\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;tag\u0026#34; href=\u0026#34;/tag/thinking/page/1/\u0026#34;\u0026gt;thinking\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;tag\u0026#34; href=\u0026#34;/tag/world/page/1/\u0026#34;\u0026gt;world\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 通过response的css方法访问div标签且class为\u0026quot;quote\u0026quot;的部分，返回的应该是一个列表，因为每句话都是这样的形式，上面只是其中的一句话。\nresponse.css(\u0026#34;div.quote\u0026#34;) [\u0026lt;Selector xpath=\u0026#34;descendant-or-self::div[@class and contains(concat(\u0026#39; \u0026#39;, normalize-space(@class), \u0026#39; \u0026#39;), \u0026#39; quote \u0026#39;)]\u0026#34; data=\u0026#39;\u0026lt;div class=\u0026#34;quote\u0026#34; itemscope itemtype...\u0026#39;\u0026gt;, \u0026lt;Selector xpath=\u0026#34;descendant-or-self::div[@class and contains(concat(\u0026#39; \u0026#39;, normalize-space(@class), \u0026#39; \u0026#39;), \u0026#39; quote \u0026#39;)]\u0026#34; data=\u0026#39;\u0026lt;div class=\u0026#34;quote\u0026#34; itemscope itemtype...\u0026#39;\u0026gt;, ...] 可以选择第一句\nquote = response.css(\u0026#34;div.quote\u0026#34;)[0] 接着可以进行提取\ntext = quote.css(\u0026#34;span.text::text\u0026#34;).get() \u0026gt;\u0026gt;\u0026gt; text \u0026#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\u0026#39; \u0026gt;\u0026gt;\u0026gt; author = quote.css(\u0026#34;small.author::text\u0026#34;).get() \u0026gt;\u0026gt;\u0026gt; author \u0026#39;Albert Einstein\u0026#39; 获得tags下面的所有tag\ntags = quote.css(\u0026#34;div.tags a.tag::text\u0026#34;).getall() \u0026gt;\u0026gt;\u0026gt; tags [\u0026#39;change\u0026#39;, \u0026#39;deep-thoughts\u0026#39;, \u0026#39;thinking\u0026#39;, \u0026#39;world\u0026#39;] 上面尝试完了shell中的运行，接下来可以试着把写一个脚本来实现\nfor quote in response.css(\u0026#34;div.quote\u0026#34;): ... text = quote.css(\u0026#34;span.text::text\u0026#34;).get() ... author = quote.css(\u0026#34;small.author::text\u0026#34;).get() ... tags = quote.css(\u0026#34;div.tags a.tag::text\u0026#34;).getall() ... print(dict(text=text, author=author, tags=tags)) {\u0026#39;text\u0026#39;: \u0026#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\u0026#39;, \u0026#39;author\u0026#39;: \u0026#39;Albert Einstein\u0026#39;, \u0026#39;tags\u0026#39;: [\u0026#39;change\u0026#39;, \u0026#39;deep-thoughts\u0026#39;, \u0026#39;thinking\u0026#39;, \u0026#39;world\u0026#39;]} {\u0026#39;text\u0026#39;: \u0026#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”\u0026#39;, \u0026#39;author\u0026#39;: \u0026#39;J.K. Rowling\u0026#39;, \u0026#39;tags\u0026#39;: [\u0026#39;abilities\u0026#39;, \u0026#39;choices\u0026#39;]} ... 循环实现所有的quote的爬取\n回到我们之前最早的例程中，改写如下：\nimport scrapy class QuotesSpider(scrapy.Spider): name = \u0026#34;quotes\u0026#34; start_urls = [ \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39;, \u0026#39;http://quotes.toscrape.com/page/2/\u0026#39;, ] def parse(self, response): for quote in response.css(\u0026#39;div.quote\u0026#39;): yield { \u0026#39;text\u0026#39;: quote.css(\u0026#39;span.text::text\u0026#39;).get(), \u0026#39;author\u0026#39;: quote.css(\u0026#39;small.author::text\u0026#39;).get(), \u0026#39;tags\u0026#39;: quote.css(\u0026#39;div.tags a.tag::text\u0026#39;).getall(), } 正如官方文档写的那样：\nIf you run this spider, it will output the extracted data with the log:\n2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from \u0026lt;200 http://quotes.toscrape.com/page/1/\u0026gt; {'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'} 2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from \u0026lt;200 http://quotes.toscrape.com/page/1/\u0026gt; {'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': \u0026quot;“I have not failed. I've just found 10,000 ways that won't work.”\u0026quot;} Storing the scraped data 最简单的方法是用Feed exports，例如json、json line、xml、csv等\nscrapy crawl quotes -O quotes.json That will generate a quotes.json file containing all scraped items, serialized in JSON.\nThe -O command-line switch overwrites any existing file; use -o instead to append new content to any existing file**(个人感觉是json外层有一对中括号的缘故，而json line没有，而是\u0026quot;流式\u0026quot;的)**.However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines:\nscrapy crawl quotes -o quotes.jl Following links Q：当你不满足于当前页的爬取，想要利用nextpage按钮来跳转到下一页进行自动爬取的时候该怎么办？\n观察到跳转按键如下\n\u0026lt;ul class=\u0026#34;pager\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;next\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/page/2/\u0026#34;\u0026gt;Next \u0026lt;span aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026amp;rarr;\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 我们在shell中输入下面的命令\nresponse.css(\u0026#39;li.next a\u0026#39;).get() \u0026#39;\u0026lt;a href=\u0026#34;/page/2/\u0026#34;\u0026gt;Next \u0026lt;span aria-hidden=\u0026#34;true\u0026#34;\u0026gt;→\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026#39; This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n\u0026gt;\u0026gt;\u0026gt; response.css('li.next a::attr(href)').get() '/page/2/' There is also an attrib property available (see Selecting element attributes for more):\n\u0026gt;\u0026gt;\u0026gt; response.css(\u0026#39;li.next a\u0026#39;).attrib[\u0026#39;href\u0026#39;] \u0026#39;/page/2/\u0026#39; Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:\nimport scrapy class QuotesSpider(scrapy.Spider): name = \u0026#34;quotes\u0026#34; start_urls = [ \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39;, ] def parse(self, response): for quote in response.css(\u0026#39;div.quote\u0026#39;): yield { \u0026#39;text\u0026#39;: quote.css(\u0026#39;span.text::text\u0026#39;).get(), \u0026#39;author\u0026#39;: quote.css(\u0026#39;small.author::text\u0026#39;).get(), \u0026#39;tags\u0026#39;: quote.css(\u0026#39;div.tags a.tag::text\u0026#39;).getall(), } next_page = response.css(\u0026#39;li.next a::attr(href)\u0026#39;).get() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) 通过urljoin的方式进行url的拼接，然后得到下一页，利用回调函数慢慢一页页搞下去\n对于urljoin的个人理解：不是普通的拼接，他应该会根据原来url中的'/\u0026lsquo;和next_page中的url的\u0026rsquo;/\u0026lsquo;进行对比，从后往前根据\u0026rsquo;/\u0026lsquo;去匹配到一个合适的位置进行替换\nA shortcut for creating Requests 利用response.follow来代替scrapy.Request\nimport scrapy class QuotesSpider(scrapy.Spider): name = \u0026#34;quotes\u0026#34; start_urls = [ \u0026#39;http://quotes.toscrape.com/page/1/\u0026#39;, ] def parse(self, response): for quote in response.css(\u0026#39;div.quote\u0026#39;): yield { \u0026#39;text\u0026#39;: quote.css(\u0026#39;span.text::text\u0026#39;).get(), \u0026#39;author\u0026#39;: quote.css(\u0026#39;span small::text\u0026#39;).get(), \u0026#39;tags\u0026#39;: quote.css(\u0026#39;div.tags a.tag::text\u0026#39;).getall(), } next_page = response.css(\u0026#39;li.next a::attr(href)\u0026#39;).get() if next_page is not None: yield response.follow(next_page, callback=self.parse) 对比两种方法，前者省略了urljoin的过程：\nif next_page is not None: yield response.follow(next_page, callback=self.parse) if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) You can also pass a selector to response.follow instead of a string; this selector should extract necessary attributes:\nfor href in response.css('ul.pager a::attr(href)'): yield response.follow(href, callback=self.parse) For \u0026lt;a\u0026gt; elements there is a shortcut: response.follow uses their href attribute automatically. So the code can be shortened further:\nfor a in response.css(\u0026#39;ul.pager a\u0026#39;): yield response.follow(a, callback=self.parse) To create multiple requests from an iterable, you can use response.follow_all instead:\nanchors = response.css(\u0026#39;ul.pager a\u0026#39;) yield from response.follow_all(anchors, callback=self.parse) or, shortening it further:\nyield from response.follow_all(css=\u0026#39;ul.pager a\u0026#39;, callback=self.parse) More examples and patterns  一堆懒狗精简用法，主要是用了一次就yield一下把数据扔了，直接看吧不解释了主要是懒\n Here is another spider that illustrates callbacks and following links, this time for scraping author information:\nimport scrapy class AuthorSpider(scrapy.Spider): name = \u0026#39;author\u0026#39; start_urls = [\u0026#39;http://quotes.toscrape.com/\u0026#39;] def parse(self, response): author_page_links = response.css(\u0026#39;.author + a\u0026#39;) yield from response.follow_all(author_page_links, self.parse_author) pagination_links = response.css(\u0026#39;li.next a\u0026#39;) yield from response.follow_all(pagination_links, self.parse) def parse_author(self, response): def extract_with_css(query): return response.css(query).get(default=\u0026#39;\u0026#39;).strip() yield { \u0026#39;name\u0026#39;: extract_with_css(\u0026#39;h3.author-title::text\u0026#39;), \u0026#39;birthdate\u0026#39;: extract_with_css(\u0026#39;.author-born-date::text\u0026#39;), \u0026#39;bio\u0026#39;: extract_with_css(\u0026#39;.author-description::text\u0026#39;), } This spider will start from the main page, it will follow all the links to the authors pages calling the parse_author callback for each of them, and also the pagination links with the parse callback as we saw before.\nHere we’re passing callbacks to response.follow_all as positional arguments to make the code shorter; it also works for Request.\nThe parse_author callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data.\nAnother interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.\nHopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.\nAs yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.\nAlso, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks.\nUsing spider arguments You can provide command line arguments to your spiders by using the -a option when running them:\nscrapy crawl quotes -O quotes-humor.json -a tag=humor These arguments are passed to the Spider’s __init__ method and become spider attributes by default.\nIn this example, the value provided for the tag argument will be available via self.tag. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:\nimport scrapy class QuotesSpider(scrapy.Spider): name = \u0026quot;quotes\u0026quot; def start_requests(self): url = 'http://quotes.toscrape.com/' tag = getattr(self, 'tag', None) if tag is not None: url = url + 'tag/' + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css('div.quote'): yield { 'text': quote.css('span.text::text').get(), 'author': quote.css('small.author::text').get(), } next_page = response.css('li.next a::attr(href)').get() if next_page is not None: yield response.follow(next_page, self.parse) If you pass the tag=humor argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as http://quotes.toscrape.com/tag/humor.\nYou can learn more about handling spider arguments here.\nNext steps This tutorial covered only the basics of Scrapy, but there’s a lot of other features not mentioned here. Check the What else? section in Scrapy at a glance chapter for a quick overview of the most important ones.\nYou can continue from the section Basic concepts to know more about the command-line tool, spiders, selectors and other things the tutorial hasn’t covered like modeling the scraped data. If you prefer to play with an example project, check the Examples section.\n","date":"2021-11-14T19:32:57+08:00","image":"https://example.com/p/scrapy/1_hucab7db9daa2d52616108d2452d926035_51257_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/scrapy/","title":"Scrapy"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用  思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n 图片  Photo by Florian Klauer on Unsplash   Photo by Luca Bravo on Unsplash \n Photo by Helena Hertz on Unsplash   Photo by Hudai Gayiran on Unsplash \n![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://example.com/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.— Rob Pike1 Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Italics Bold Code     italics bold code    Code Blocks Code block with backticks \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Fruit  Apple Orange Banana   Dairy  Milk Cheese    Other Elements — abbr, sub, sup, kbd, mark GIFis a bitmap image format.\nH2O\nXn+ Yn= ZnPress CTRL+ALT+Deleteto end the session.\nMost salamandersare nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-11T00:00:00Z","image":"https://example.com/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n YouTube Privacy Enhanced Shortcode    Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019  Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }   bilibilibi Shortcode \r","date":"2019-03-10T00:00:00Z","permalink":"https://example.com/p/rich-content/","title":"Rich Content"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://example.com/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu958d513eeefe5556a31d065479ecc5ac_14205_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }}  To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files  Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://example.com/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","date":"2019-03-05T00:00:00Z","image":"https://example.com/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_huf941de4769045cdfa8c9ee7036519a2a_35369_120x120_fill_q75_box_smart1.jpg","permalink":"https://example.com/p/emoji-support/","title":"Emoji Support"}]